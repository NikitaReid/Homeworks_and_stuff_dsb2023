---
title: "Homework 4: Machine Learning"
author: "Nikita Reid"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---

```{r label: load-libraries, message=FALSE, warning=FALSE}

options(scipen = 999) #disable scientific notation
library(tidyverse)
library(tidymodels)
library(GGally)
library(sf)
library(leaflet)
library(janitor)
library(rpart.plot)
library(here)
library(scales)
library(vip)

```

# The Bechdel Test

The [Bechdel test](https://bechdeltest.com) is a way to assess how women are depicted in Hollywood movies. For a movie to pass the test:

1.  It has to have at least two [named] women in it
2.  Who talk to each other
3.  About something besides a man

We have a sample of 1394 movies and we want to fit a model to predict whether a film passes the test or not.

```{r read_data}

# load data and avoid a messy output 
bechdel <- read_csv(here::here("data", "bechdel.csv"),
                    show_col_types = FALSE) %>% 
  
# turn "test" into a factor variable 
  mutate(test = factor(test)) 

# look at the data set 
glimpse(bechdel)

```

How many films fail/pass the test, both as a number and as a %?

```{r pass_the_test}

bechdel %>% 
# count how many films pass/fail the test
  count(test) %>% 
  
# create column to calculate % for pass/fail 
  mutate(percentage = n/sum(n)*100)

```

## Movie scores

The below plot shows the distribution of film ratings by IMDB and Metacritic rating sites and whether these films passed or failed the Bechdel test. There is clearly no relationship between film ratings and whether films pass or fail the Bechdel test.

```{r movie-ratings-plot}

# plot ratings data coloured by passed and failed tests 
ggplot(data = bechdel, aes(
  x = metascore,
  y = imdb_rating,
  colour = test
)) +
  
# specify some peculiar/fun colours and 
# add some labels and theme 
  geom_point(alpha = .3, size = 3) +
  scale_colour_manual(values = c("tomato", "olivedrab")) +
  labs(
    x = "Metacritic score",
    y = "IMDB rating",
    colour = "Bechdel test"
  ) +
 theme_light()

```

# Split the data

```{r split_data}

set.seed(123)

# 80% of data for training , 20% for testing 
# make sure equal proportions of pass/fail test in each group 

data_split <- initial_split(bechdel, # updated data
                           prop = 0.8, 
                           strata = test)

# split the data

bechdel_train <- training(data_split) 
bechdel_test <- testing(data_split)
```

Check the counts and % proportions of the `test` variable in each set. We have pretty much equal proportions of pass/fail the test in each of the two sets (training/testing). This is good!

```{r checking_proportions}

# first check the training set 
bechdel_train %>% 
  
# count how many films pass/fail the test
  count(test) %>% 
  
# create column to calculate % for pass/fail 
  mutate(percentage = n/sum(n)*100)

# now check the testing set 
bechdel_test %>% 
  
# count how many films pass/fail the test
  count(test) %>% 
  
# create column to calculate % for pass/fail 
  mutate(percentage = n/sum(n)*100)
```

## Feature exploration

### Any outliers?

There are no "unusual" or "impossible" outliers to speak of. There are some very low film ratings as seen below for metascore and imdb rating. However, this probably just means they were bad movies. Similarly, there are some movies with very high budgets/ box office grosses but this is also reasonable to expect. As a basic sanity check, budget outlier values \< domestic gross outlier values \< international gross outlier values, as expected. So, none of these outliers appear to be problematic.

```{r outliers}

# select desired columns and pivot longer to put all variables in one column 
# and their values in another 
bechdel %>% 
  select(test, budget_2013, domgross_2013, intgross_2013, imdb_rating, metascore) %>% 

    pivot_longer(cols = 2:6,
               names_to = "feature",
               values_to = "value") %>% 
# plot boxplot of all features/variables to check for outlier values 
  ggplot()+
  aes(x=test, y = value, fill = test)+
  coord_flip()+
  geom_boxplot()+

# split plots by feature 
  facet_wrap(~feature, scales = "free")+
  theme_bw()+
  theme(legend.position = "none")+
  labs(x=NULL,y = NULL)

```

### Scatter plot - Correlation Matrix

In the below scatter plot, blue corresponds to passing the Bechdel test and pink corresponds to failing it. More films fail the Bechdel test than pass it. There is no large difference between films that pass or fail the test and their budgets or ratings except that as a general rule, budget and box office take medians seem to be slightly larger for films that fail the Bechdel test but not by much. The reason for this is unclear. Budgets and grosses are quite skewed distributions towards lower values, with a long tail of a few high budget films. This makes sense as most movies do not have large budgets and budgets tend to be highly positively correlated to box office grosses as well (we see this here with budget variable having a correlation of roughly +0.52 and +0.62 with domestic gross and international gross respectively). Domestic and international gross are also extremely positively correlated here (\~ +0.94). Ratings are fairly normally distributed, which makes sense as one would expect the bulk of films to be about average in quality and mass appeal. Ratings are in no way affected by whether the Bechdel test is passed or failed. Ratings (both ratings variables) have no correlation to budget, weakly positive correlation to gross (international and domestic) and imdb rating and metascore have a very high correlation to each other of \~ +0.74, as one would expect. Correlations between any of the other variables do not seem to be much affected by whether a film passes or fails the Bechdel test.

```{r correlations, warning=FALSE, message=FALSE, fig.height=8, fig.width=12}
bechdel %>% 
  
# select desired variable columns 
# plot scatterplot of variable correlations using ggpairs 
  
  select(test, budget_2013, domgross_2013, intgross_2013, imdb_rating, metascore)%>% 
  ggpairs(aes(colour=test), alpha=0.2)+
  theme_bw()
```

### Categorical variables

Below we see that musicals always pass the Bechdel test, and horror movies frequently pass them (about 70% of the time). This makes sense as these genres tend to incorporate many themes and do not tend to focus only on men or women discussing men. Crime, action and adventure films rarely pass the Bechdel test. These genres often tend to focus more on male characters and so this would make sense. Comedies, biographies and dramas have about equal shares of passing/failing which also makes sense.

There does not seem to be much of a relationship between age rating of films and whether or not they pass the Bechdel test with all respective proportions being about 50/50 or at worst 60/40 which is similar to the overall proportions of films failing/passing the test. Only the NC-17 age category stands out with very few films passing the test, however there are only 6 films in this category in total for this sample data set and so we probably cannot draw any conclusions with such a small sample.

```{r genres_and_test, message=FALSE, warning=FALSE}

# group by genre / Bechdel test 
# count number and proportion of films that pass/ fail the test for every genre 
bechdel %>% 
  group_by(genre, test) %>%
  summarise(n = n()) %>% 
  mutate(prop = n/sum(n))
  
# group by age rating / Bechdel test 
# count number and proportion of films that pass/ fail the test for every age rating  
bechdel %>% 
  group_by(rated, test) %>%
  summarise(n = n()) %>% 
  mutate(prop = n/sum(n))
```

# Train first models. `test ~ metascore + imdb_rating`

```{r setting_models}

# regression model with classifaction mode 
lr_mod <- logistic_reg() %>% 
  set_engine(engine = "glm") %>% 
  set_mode("classification")

lr_mod

# decision tree model with classification mode 
tree_mod <- decision_tree() %>% 
  set_engine(engine = "C5.0") %>% 
  set_mode("classification")

tree_mod 
```

```{r fitting_models}


lr_fit <- lr_mod %>% # parsnip model
  fit(test ~ metascore + imdb_rating, # a formula
    data = bechdel_train # dataframe
  )

tree_fit <- tree_mod %>% # parsnip model
  fit(test ~ metascore + imdb_rating, # a formula
    data = bechdel_train # dataframe
  )
```

## Logistic regression

```{r fit-logistic}

lr_fit %>%
  broom::tidy()

lr_preds <- lr_fit %>%
  augment(new_data = bechdel_train) %>%
  mutate(.pred_match = if_else(test == .pred_class, 1, 0))

```

### Confusion matrix

```{r confusion_logistic}

lr_preds %>% 
  conf_mat(truth = test, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")

```

137 false positives, 328 false negatives (almost double the number of true positives). This model makes bad predictions ....

## Decision Tree

```{r fit-tree}

tree_preds <- tree_fit %>%
  augment(new_data = bechdel) %>%
  mutate(.pred_match = if_else(test == .pred_class, 1, 0)) 

```

```{r confusion-tree}

tree_preds %>% 
  conf_mat(truth = test, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")

```

189 false positives, 397 false negatives (almost double the number of true positives). This model also makes bad predictions ....

## Draw the decision tree

```{r draw-tree, fig.height=16, fig.width=18}

# draw our fitted decision tree 
draw_tree <- 
    rpart::rpart(
        test ~ metascore + imdb_rating,
        data = bechdel_train, 
        control = rpart::rpart.control(maxdepth = 5, cp = 0, minsplit = 10)
    ) %>% 
    partykit::as.party()
plot(draw_tree)

```

# Cross Validation

The code below return the IDs of the 10 folds we created (10 random splits of the data set each containing roughly equal proportions of pass/fail results for the test variable).

```{r vfolds, message=FALSE, warning=FALSE}

set.seed(123)
bechdel_folds <- vfold_cv(data = bechdel_train, 
                          v = 10, 
                          strata = test)
bechdel_folds
```

Train and test a resampled model:

```{r resamples}

# resample the data using fit_resamples
# training the two models using the ten folds we created 

lr_fit <- lr_mod %>%
  fit_resamples(
    test ~ metascore + imdb_rating,
    resamples = bechdel_folds
  )


tree_fit <- tree_mod %>%
  fit_resamples(
    test ~ metascore + imdb_rating,
    resamples = bechdel_folds
  )
```

Unnest the metrics column - how accurately does the model predict whether test is passed? :

```{r collect_metrics}

collect_metrics(lr_fit)
collect_metrics(tree_fit)


```

```{r test_the_models}

tree_preds <- tree_mod %>% 
  fit_resamples(
    test ~ metascore + imdb_rating, 
    resamples = bechdel_folds,
    control = control_resamples(save_pred = TRUE) #<<
  )

# What does the data for ROC look like?
tree_preds %>% 
  collect_predictions() %>% 
  roc_curve(truth = test, .pred_Fail)  

# Draw the ROC
tree_preds %>% 
  collect_predictions() %>% 
  roc_curve(truth = test, .pred_Fail) %>% 
  autoplot()

```

What we learn here after training and testing the initial logistic regression and decision tree models and using 10 v_folds for resampling is that both models predict the test outcome correctly roughly 50-60% of the time (see area under ROC curves metrics and ROC curve plotted for decision tree above). This is terrible! We might as well flip a coin...

# Build a better training set with `recipes`

## Collapse Some Categorical Levels

Do we have any `genre` with few observations? Assign genres that have less than 3% to a new category 'Other'. We do: Mystery, Fantasy, Sci-Fi, Thriller, Documentary, Musical.

```{r genre-plot}

# plot number of films in each genre 
# reorder with largest bar on top (highest number of films)
bechdel %>% 
  count(genre) %>% 
  mutate(genre = fct_reorder(genre, n)) %>% 
  ggplot(aes(x = genre, 
             y = n)) +
  geom_col(alpha = .8) +
  coord_flip() +
  labs(x = NULL) +
  geom_hline(yintercept = (nrow(bechdel_train)*.03), lty = 3)+
  theme_light()

```

```{r collapse-categories}

movie_rec <-
  recipe(test ~ .,
         data = bechdel_train) %>%
  
# Genres with less than 3% will be in a category 'Other'
    step_other(genre, threshold = .03) 

```

Before recipe:

```{r before-recipe}

bechdel_train %>% 
  count(genre, sort = TRUE)

```

After recipe (small categories collapsed to "other") :

```{r after-recipe}

movie_rec %>% 
  prep() %>% 
  bake(new_data = bechdel_train) %>% 
  count(genre, sort = TRUE)

```

Converts nominal data into numeric dummy variables using step_dummy() :

```{r step_dummy,  results='hide' , message=FALSE}

movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_dummy(all_nominal_predictors()) 

movie_rec 
```

## Let's think about the modelling

What if there were no films with `rated` NC-17 in the training data?

-   Will the model have a coefficient for `rated` NC-17? No!
-   What will happen if the test data includes a film with `rated` NC-17? It will not know what to do!

Soltuion: step_novel() adds a catch-all level to a factor for any new values not encountered in model training, which lets R intelligently predict new levels in the test set.

```{r step_novel}

movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_novel(all_nominal_predictors) %>% # Use *before* `step_dummy()` so new level is dummified
  step_dummy(all_nominal_predictors()) 

```

step_zv() Intelligently handles zero variance variables (variables that contain only a single value) :

```{r step_zv}

movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% # Use *before* `step_dummy()` so new level is dummified
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_numeric(), -all_outcomes()) 
  
```

step_normalize() centers then scales numeric variable (mean = 0, sd = 1) :

```{r step_normalize}
movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% # Use *before* `step_dummy()` so new level is dummified
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_numeric(), -all_outcomes())  %>% 
  step_normalize(all_numeric()) 

```

Lastly, step_corr() removes highly correlated variables:

```{r step_corr, message=FALSE}

movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% # Use *before* `step_dummy()` so new level is dummified
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_numeric(), -all_outcomes())  %>% 
  step_normalize(all_numeric()) %>% 
  step_corr(all_predictors(), threshold = 0.75, method = "spearman") 

movie_rec

```

# Define different models to fit

```{r model_building, message=FALSE}

# Pick a `model type`
# set the `engine`
# Set the `mode`: regression or classification
# we do this for five different model types below 
# and use only classification mode

# Logistic regression
log_spec <-  logistic_reg() %>%  # model type
  set_engine(engine = "glm") %>%  # model engine
  set_mode("classification") # model mode

# Show your model specification
log_spec

# Decision Tree
tree_spec <- decision_tree() %>%
  set_engine(engine = "C5.0") %>%
  set_mode("classification")

tree_spec

# Random Forest
library(ranger)

rf_spec <- 
  rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")


# Boosted tree (XGBoost)
library(xgboost)

xgb_spec <- 
  boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") 

# K-nearest neighbour (k-NN)
knn_spec <- 
  nearest_neighbor(neighbors = 4) %>% # we can adjust the number of neighbors 
  set_engine("kknn") %>% 
  set_mode("classification") 

```

# Bundle recipe and model with `workflows`

```{r workflows}

log_wflow <- # new workflow object
 workflow() %>% # use workflow function
 add_recipe(movie_rec) %>%   # use the new recipe
 add_model(log_spec)   # add your model spec

# show object
log_wflow


# workflows for the other four models 

tree_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(tree_spec) 

rf_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(rf_spec) 

xgb_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(xgb_spec)

knn_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(knn_spec)

```

1.  How many models have you specified?

    Specified above are five different models of type: logistic regression, decision tree, random forest, boosted tree and K-nearest neighbors.

2.  What's the difference between a model specification and a workflow?

    A model specification is simply the specification of a parsnip model that contains the model type, engine and mode. A workflow object is a combination of a pre-processor (here our 6 step recipe) and a parsnip model specification (here any one of our five models we specified above).

3.  Do you need to add a formula (e.g., `test ~ .`) if you have a recipe?

    No. The recipe pre-processor already incorporates these formulas.

# Model Comparison

You now have all your models. Adapt the code from slides `code-from-slides-CA-housing.R`, line 400 onwards to assess which model gives you the best classification.

```{r}

```
