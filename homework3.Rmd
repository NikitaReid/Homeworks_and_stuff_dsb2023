---
title: "Homework 3: Databases and web scraping"
author: "Nikita Reid"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---

```{r load-libraries,echo=FALSE,message=FALSE,warning=FALSE}

# load required libraries 

library(tidyverse)
library(wbstats)
library(tictoc)
library(skimr)
library(countrycode)
library(here)
library(DBI)
library(dbplyr)
library(arrow)
library(rvest)
library(robotstxt) # check if we're allowed to scrape the data
library(scales)
library(sf)
library(readxl)

```

# Money in UK politics

[The Westminster Accounts](https://news.sky.com/story/the-westminster-accounts-12786091), a recent collaboration between Sky News and Tortoise Media, examines the flow of money through UK politics. It does so by combining data from three key sources:

1.  [Register of Members' Financial Interests](https://www.parliament.uk/mps-lords-and-offices/standards-and-financial-interests/parliamentary-commissioner-for-standards/registers-of-interests/register-of-members-financial-interests/),
2.  [Electoral Commission records of donations to parties](http://search.electoralcommission.org.uk/English/Search/Donations), and
3.  [Register of All-Party Parliamentary Groups](https://www.parliament.uk/mps-lords-and-offices/standards-and-financial-interests/parliamentary-commissioner-for-standards/registers-of-interests/register-of-all-party-party-parliamentary-groups/).

The database, made available by Simon Willison, is an `SQLite` database.

```{r opening-connection-to-database}

# establish connection to database 
# specify driver and database name 
sky_westminster <- DBI::dbConnect(
  drv = RSQLite::SQLite(),
  dbname = here::here("data", "sky-westminster-files.db")
)
```

How many tables does the database have? There are 7 tables listed.

```{r list-tables}

# list all tables in the database 
DBI::dbListTables(sky_westminster)
```

Which MP has received the most amount of money? Theresa May.

```{r mp-most-money}
 
# set up database objects with dplyr for payments, members
# rename id column in members so we have a common member_id column to join the two tables by
members <- tbl(sky_westminster, "members") %>% 
  rename(member_id = id)

payments <- tbl(sky_westminster, "payments")

# join the tables to get MP names & payment values in same table 
member_payments <- left_join(x=payments, y=members, by = 'member_id')

member_payments %>% 
# group by MP name 
  group_by(name) %>% 
# calculate total money received by each MP 
  summarise(total_money = sum(value)) %>% 
# arrange to get MP who received most money to top of the table 
  arrange(desc(total_money))

```

Is there any `entity` whose donations account for more than 5% of the total payments given to MPs over the 2020-2022 interval? Who are they and who did they give money to?

If we exclude payments made to MPs as "Earnings from other employment" , as I think we should because these are not strictly "donations" , then no entity comes in above 5%. The largest donating entity is Unite with roughly 3.8% of donations in this time period (see first table). Investigating, it seems they donated to a large number of MPs, but donated more than 4 times more to Rebecca Long-Bailey than anybody else (see second table).

If we then go back and include all payment categories, one entity, Withers LLP comes in at roughly 5.8% of total payments to MPs (see third table) and all of these payments were made to a single MP - Sir Geoffrey Cox (see fourth table).

```{r big-donations-entities, message=FALSE, warning=FALSE}

# first we filter and examine only donations (not other earnings)
member_payments %>%
  filter(category_name != "Earnings from other employment") %>%
# collect data into R
  collect() %>% 
  
# separate out date column so we can filter for years 2020-2022 only
  separate(date, into= c("word1", "word2", "month", "year"),sep=" " ) %>% 
  filter(year=="2020" | year=="2021" | year=="2022"  ) %>% 
  
# group by entity and remove missing values 
  group_by(entity) %>% 
  drop_na(value) %>% 
  
# calculate total donations and percentage of total for each entity
# arranged largest to smallest
  summarize(total_donations=sum(value)) %>% 
  arrange(desc(total_donations)) %>% 
  mutate(percentage=total_donations/sum(total_donations)*100)

# investigate only highest % entity - Unite   
member_payments %>% 
  filter(entity=="Unite") %>% 
  
# find out which MPs they donated to and how much to each MP in total
# arranged largest to smallest  
  group_by(name) %>% 
  summarize(total_donations_Unite=sum(value)) %>% 
  arrange(desc(total_donations_Unite))


# now we examine all payments 
member_payments %>%
# collect data into R
  collect() %>% 
  
# separate out date column so we can filter for years 2020-2022 only  
  separate(date, into= c("word1", "word2", "month", "year"),sep=" " ) %>% 
  filter(year=="2020" | year=="2021" | year=="2022"  ) %>% 
  
# group by entity and remove missing values 
  group_by(entity) %>% 
  drop_na(value) %>% 

# calculate total payments and percentage of total for each entity
# arranged largest to smallest
  summarize(total_payments=sum(value)) %>% 
  arrange(desc(total_payments)) %>% 
  mutate(percentage=total_payments/sum(total_payments)*100)

# investigate only highest % entity - Withers
member_payments %>% 
  filter(entity=="Withers LLP") %>% 
  
# find out which MPs they donated to and how much to each MP in total
  group_by(name) %>% 
  summarize(total_payments_Withers=sum(value)) 

```

How many distinct entities who paid money to MPS are there?

The authors of the database explicitly state that they have taken care to eliminate duplicates and combine names/entities etc. that they believe to be the same. Hence, I will not brute force any of the entities that "look similar" together because we cannot definitively know they are the same. Hence, 2213 unique entities.

```{r distinct_entities}

# collect data into R 
# and count number of unique entity data values 
member_payments %>% 
  collect() %>% 
  summarise(number_of_unique_entities = n_unique(entity))

```

How many (as a number and %) donated to MPs belonging to a single party only?

2036 entities (\~92%)

```{r single-party-only}

# group by entity and collect data into R
member_payments %>% 
  group_by(entity) %>% 
  collect() %>% 
  
# calculate number of parties each entity donated to
  summarize(number_of_parties = n_unique(party_id)) %>% 
  
# create a column to check whether they only donated to a single party id 
# and count single party yes versus no 
  mutate(single_party = case_when(number_of_parties==1 ~"yes", TRUE ~"no")) %>% 
  count(single_party) %>% 
  
# calculate percentages 
  mutate(percentage = n/sum(n)*100) %>% 
  
# only display results for entities donating to a single party 
  filter(single_party=="yes")
  
```

## Which party has raised the greatest amount of money in each of the years 2020-2022?

```{r party-donations-table, message=FALSE}

# reproducing the figure requested (example figure has been removed here)

# set up database objects with dplyr for donations, parties
# rename id column in parties so we have a common party_id column to join the two tables by
party_donations <- tbl(sky_westminster, "party_donations")

parties <- tbl(sky_westminster, "parties") %>% 
  rename(party_id = id)

# join tables to get party names & donation values in same table
party_table <- left_join(x=party_donations, y=parties, by="party_id")

party_table %>% 
# use lubridate to extract only year value out of date 
  mutate(year=lubridate::year(date)) %>% 
  
# group by year and party 
  group_by(year,name) %>% 
  
# calculate total donations and proportion for each party & year
  summarise(total_year_donations=sum(value)) %>% 
  mutate(prop = 
           total_year_donations/sum(total_year_donations)) 

# table image reproduced 
```

```{r party-donations-graph, message=FALSE}

# and now for the graph reproduction requested ...

party_table %>% 
# use lubridate to extract only year value out of date 
  mutate(year=lubridate::year(date)) %>% 
  
# group by year and party 
  group_by(year,name) %>% 
  
# calculate total donations and proportion for each party & year
  summarise(total_year_donations=sum(value)) %>% 
  mutate(prop = 
           total_year_donations/sum(total_year_donations)) %>% 
  

# bar chart with year on x axis, total donations on y axis 
# grouped by party 

  ggplot(aes(x=year, y=total_year_donations, fill=name)) +
  
# rearrange parties according to total donations
  geom_bar(aes(fill=reorder(name,desc(total_year_donations))),
# position party bars side by side 
           position="dodge", stat = "identity") +
  
# make y-axis scientific notation go away 
  scale_y_continuous(labels = scales::comma) +
  
# fix labels and theme as in example figure 
  labs(x ="", y ="", fill="Party",
       title = "Conservatives have captured the majority of political donations",
       subtitle = "Donations to political parties, 2020-2022") +
  theme_bw()

# graph image reproduced 
```

```{r prettier-graph, message=FALSE}

# the same as above using some facetting to make things prettier (maybe?)

party_table %>% 
# use lubridate to extract only year value out of date 
  mutate(year=lubridate::year(date)) %>% 
  
# group by year and party 
  group_by(year,name) %>% 
  
# calculate total donations and proportion for each party & year
  summarise(total_year_donations=sum(value)) %>% 
  mutate(prop = 
           total_year_donations/sum(total_year_donations)) %>% 
  

# bar chart with year on x axis, total donations on y axis 

  ggplot(aes(x=year, y=total_year_donations, fill=name)) +
  
  geom_bar(position="dodge", stat = "identity") +
  
# make y-axis scientific notation go away 
  scale_y_continuous(labels = scales::comma) +
  
# fix labels and theme as in example figure 
  labs(x ="", y ="",
       title = "Conservatives have captured the majority of political donations",
       subtitle = "Donations to political parties, 2020-2022") +
  theme_bw() +
  
# now facet wrap by party, make the y scales free and remove legend 
  facet_wrap(~name, scales = "free_y") +
  guides(fill="none") 
```

```{r disconnect-database, warning=FALSE}

# disconnect from the database 
dbDisconnect(sky_westminster)
```

# Anonymised COVID patient data from the CDC

We will be using a data set with [anonymous Covid-19 patient data that the CDC publishes every month](https://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data-with-Ge/n8mc-b4w4).

```{r covid-data, echo=FALSE, message=FALSE, warning=FALSE}

tic() # start timer
cdc_data <- open_dataset(here::here("data", "cdc-covid-geography"))
toc() # stop timer

```

COVID Case Fatality Ratio (CFR %) by age group, sex and ICU Admission

```{r covid-cfr-plot-1}

# filter out all missing and unknown data responses 
cdc_data %>% 
  filter(icu_yn == "Yes"| icu_yn == "No") %>%
  filter(sex=="Male" | sex=="Female") %>% 
  filter(age_group != "Missing") %>% 
  filter(death_yn != "Unknown" & death_yn != "Missing") %>% 
  
# group by 4 variables: sex, age group, icu admission and death 
  group_by(sex, age_group,icu_yn,death_yn) %>% 
  
# collect data into R and remove NA values 
# calculate number and percantage of deaths (CFR %) for each group 
  summarize(number_death = n()) %>% 
  collect() %>% 
  drop_na(sex,age_group,icu_yn,death_yn) %>% 
  mutate(percentage=number_death/sum(number_death)) %>% 

# plot only those who died for the CFR % bars 
  filter(death_yn =="Yes") %>% 
  
# create column to fix y axis facet grid labels 
  mutate(icu_reformatted = case_when(icu_yn=="Yes" ~ "ICU Admission", 
                                     TRUE ~"No ICU Admission")) %>% 

# plot bars for CFR % by age_group and use default fill as in example image 
  ggplot(aes(x = percentage, y =age_group, fill=death_yn)) +
  geom_bar(stat="identity") +
  
# 2D facet grid of patient sex versus ICU admission groups 
  facet_grid(factor(icu_reformatted, levels = c("ICU Admission",

                                                "No ICU Admission"))~sex) + 
# replicate theme and labels from example picture (include caption, remove axes)
  theme_light() +
  
  labs(x="",y="", caption = "Source: CDC",
       title="Covid CFR % by age group, sex and ICU Admission") +

# fix x-axis into % labels as in example image 
  scale_x_continuous(labels=scales::percent) + 
  
# fix y-axis levels to include evn empty age groups as in example image
  scale_y_discrete(limits = c("0 - 17 years", "18 to 49 years", 
                              "50 to 64 years", "65+ years")) + 
# no legend 
  guides(fill="none") +

# add the CFR% text to bars, rounded without decimal places, coloured black 
  geom_text((aes(label=(sprintf("%0.0f",percentage*100)))),
            size=3, hjust=1, color= "black") 

# figure reproduced 

```

Now Plot Case Fatality Ratio (CFR) over time:

```{r covid-cfr-plot-2}
  
# filter out all missing and unknown data responses 
cdc_data %>% 
  filter(icu_yn == "Yes"| icu_yn == "No") %>%
  filter(sex=="Male" | sex=="Female") %>% 
  filter(age_group != "Missing") %>% 
  filter(death_yn != "Unknown" & death_yn != "Missing") %>% 
  filter(case_month != "2020-02") %>% 
  
# group by 5 variables: case date (month), sex, age group, icu admission and death 
  group_by(case_month,sex, age_group,icu_yn,death_yn) %>% 
  
# collect data into R and remove NA values 
# calculate number and percantage of deaths (CFR %) for each group 
  summarize(number_death = n()) %>% 
  collect() %>% 
  drop_na(case_month,sex,age_group,icu_yn,death_yn) %>% 
  mutate(percentage=number_death/sum(number_death)) %>% 

# plot only those who died for the CFR % points 
  filter(death_yn =="Yes") %>% 
  
# create column to fix y axis facet grid labels 
  mutate(icu_reformatted = case_when(icu_yn=="Yes" ~ "ICU Admission", 
                                     TRUE ~"No ICU Admission")) %>% 

# plot lines for CFR %  grouped by age_group 
# and use default line color as in example image 
  ggplot(aes(x = case_month, y =percentage, group=age_group, color=age_group)) +
  geom_line() +
  
# 2D facet grid of patient sex versus ICU admission groups 
  facet_grid(factor(icu_reformatted, levels = c("ICU Admission",

                                                "No ICU Admission"))~sex,
             scales = "free") + 
# replicate theme and labels from example picture (include caption, remove axes)
# vertcally orientate x axis date labels 
  theme_light() +
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) +
  
  labs(x="",y="", caption = "Source: CDC", color="Age Group",
       title="Covid CFR % by age group, sex and ICU Admission") +

# fix y-axis into % labels as in example image 
  scale_y_continuous(labels=scales::percent) + 

# add the CFR% text to points, rounded without decimal places 
  geom_text(aes(label=(sprintf("%0.0f",percentage*100))), size=3)

# figure reproduced

```

Case Fatality ratio (CFR) in different counties, according to their population:

```{r urban-rural-data}

# load urban/rural data 

urban_rural <- read_xlsx(here::here("data", "NCHSURCodes2013.xlsx")) %>% 
  janitor::clean_names() 


```

```{r cfr-plot-3}

# first rename fips code column to have same column name to join two tables by 
# and change the dbl into an int format to have same format to join by 
urban_rural_new <- urban_rural %>% 
  mutate(fips_code_new = as.integer(fips_code)) %>% 
  rename(county_fips_code = fips_code_new) 
  
# join cdc_data and urban_rural data by county fips code 
cdc_urban_rural <- left_join(x = cdc_data, y = urban_rural_new,
                             by = "county_fips_code")

# filter out all missing and unknown data responses 
# group by case date and the 2013 code (1 - 6 scale), and death y/n 
cdc_urban_rural %>%
  filter(death_yn != "Unknown" & death_yn != "Missing") %>% 
  filter(case_month!="2020-01" & case_month!="2020-02") %>% 
  group_by(case_month, x2013_code,death_yn) %>% 
  
# calculate number and % of deaths for each group 
# collect data into R and remove missing values
  summarize(number_death = n()) %>% 
  collect() %>% 
  drop_na(case_month,death_yn,x2013_code) %>% 
  mutate(percentage=number_death/sum(number_death)) %>% 
  
# plot only those who died for the CFR % points 
  filter(death_yn =="Yes") %>% 
  
# create column to fix facet wrap labels 
  mutate(code_reformatted = case_when(x2013_code=="1" ~ "1. Large central metro",
                                     x2013_code=="2" ~"2. Large fringe metro", 
                                    x2013_code== "3" ~"3. Medium metro",
                                     x2013_code=="4" ~"4. Small metropolitan",
                                     x2013_code=="5" ~"5. Micropolitan",
                                     x2013_code=="6" ~"6. Noncore")) %>% 
  
# plot lines for CFR % each month 
  ggplot(aes(x = case_month, y =percentage, group=code_reformatted,
             color= code_reformatted)) +
  geom_line() +
  
# facet wrap of county type with three rows as in image 
  facet_wrap(~code_reformatted, scales = "free_y", nrow = 3) + 
               
# replicate theme and labels from example picture (include caption, remove axes)
# vertcally orientate x axis date labels 
  theme_light() +
  theme(axis.text.x=element_text(angle=90,hjust=3,vjust=0.5)) +
  
  labs(x="",y="", caption = "Source: CDC",
       title="Covid CFR % by county population") +

# fix y-axis into % labels as in example image 
  scale_y_continuous(labels=scales::percent) + 

# add the CFR% text to points, rounded with 1 decimal place
  geom_text(aes(label=(sprintf("%0.1f",percentage*100))), size=3) +
  
# no legend 
  guides(color="none") 

# figure reproduced

```

```{r cfr-plot-4}

# filter out all missing and unknown data responses 
# group by case date and urban/rural, and death y/n 
cdc_urban_rural %>%
  filter(death_yn != "Unknown" & death_yn != "Missing") %>% 
  filter(case_month!="2020-01") %>% 
  mutate(code_urban_rural = case_when(x2013_code==1 ~"Urban",
                                     x2013_code==2 ~"Urban", 
                                    x2013_code==3 ~"Urban",
                                     x2013_code==4 ~"Urban",
                                     x2013_code==5 ~"Rural",
                                     x2013_code==6 ~"Rural")) %>% 
  group_by(death_yn,code_urban_rural, case_month) %>% 
  
# calculate number and % of deaths for each group 
# collect data into R and remove missing values
  summarize(number_death = n()) %>% 
  collect() %>% 
  drop_na(code_urban_rural,case_month,death_yn) %>% 
  mutate(percentage=number_death/sum(number_death)) %>% 
  
# plot only those who died for the CFR % points 
  filter(death_yn =="Yes") %>% 
  
# create column to fix facet wrap labels 

  
# plot lines for CFR % each month 
  ggplot(aes(x = case_month, y =percentage, group=code_urban_rural,
             color= code_urban_rural)) +
  geom_line() + 
               
# replicate theme and labels from example picture (include caption, remove axes)
# vertcally orientate x axis date labels 
  theme_light() +
  theme(axis.text.x=element_text(angle=90,hjust=3,vjust=0.5)) +
  
  labs(x="",y="", caption = "Source: CDC", color="Counties",
       title="Covid CFR % by rural and urban areas") +

# fix y-axis into % labels as in example image 
  scale_y_continuous(labels=scales::percent) + 

# add the CFR% text to points, rounded with 1 decimal place
  geom_text(aes(label=(sprintf("%0.1f",percentage*100))), 
            size=3, color="black") 
  

# figure reproduced
```

# Money in US politics

In the United States, [*"only American citizens (and immigrants with green cards) can contribute to federal politics, but the American divisions of foreign companies can form political action committees (PACs) and collect contributions from their American employees."*](https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs)

We will scrape and work with data foreign connected PACs that donate to US political campaigns. The data for foreign connected PAC contributions in the 2022 election cycle can be found at <https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022>. Then, we will use a similar approach to get data such contributions from previous years so that we can examine trends over time.

All data come from [OpenSecrets.org](https://www.opensecrets.org), a *"website tracking the influence of money on U.S. politics, and how that money affects policy and citizens' lives"*.

```{r, eval=FALSE}
#| label: allow-scraping-opensecrets
#| warning: false
#| message: false

library(robotstxt)
paths_allowed("https://www.opensecrets.org")

base_url <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"

contributions_tables <- base_url %>%
  read_html() 

```

-   First, make sure you can scrape the data for 2022. Use janitor::clean_names() to rename variables scraped using `snake_case` naming.

-   Clean the data:

    -   Write a function that converts contribution amounts in `total`, `dems`, and `repubs` from character strings to numeric values.
    -   Separate the `country_of_origin_parent_company` into two such that country and parent company appear in different columns for country-level analysis.

```{r, eval=FALSE}
# write a function to parse_currency
parse_currency <- function(x){
  x %>%
    
    # remove dollar signs
    str_remove("\\$") %>%
    
    # remove all occurrences of commas
    str_remove_all(",") %>%
    
    # convert to numeric
    as.numeric()
}

# clean country/parent co and contributions 
contributions <- contributions %>%
  separate(country_of_origin_parent_company, 
           into = c("country", "parent"), 
           sep = "/", 
           extra = "merge") %>%
  mutate(
    total = parse_currency(total),
    dems = parse_currency(dems),
    repubs = parse_currency(repubs)
  )
```

-   Write a function called `scrape_pac()` that scrapes information from the Open Secrets webpage for foreign-connected PAC contributions in a given year. This function should

    -   have one input: the URL of the webpage and should return a data frame.
    -   add a new column to the data frame for `year`. We will want this information when we ultimately have data from all years, so this is a good time to keep track of it. Our function doesn't take a year argument, but the year is embedded in the URL, so we can extract it out of there, and add it as a new column. Use the `str_sub()` function to extract the last 4 characters from the URL. You will probably want to look at the help for this function to figure out how to specify "last 4 characters".

-   Define the URLs for 2022, 2020, and 2000 contributions. Then, test your function using these URLs as inputs. Does the function seem to do what you expected it to do?

-   Construct a vector called `urls` that contains the URLs for each webpage that contains information on foreign-connected PAC contributions for a given year.

-   Map the `scrape_pac()` function over `urls` in a way that will result in a data frame called `contributions_all`.

-   Write the data frame to a csv file called `contributions-all.csv` in the `data` folder.

# Scraping consulting jobs

The website [https://www.consultancy.uk/jobs/](https://www.consultancy.uk/jobs) lists job openings for consulting jobs.

```{r}
#| label: consulting_jobs_url
#| eval: false

library(robotstxt)
paths_allowed("https://www.consultancy.uk") #is it ok to scrape?

base_url <- "https://www.consultancy.uk/jobs/page/1"

listings_html <- base_url %>%
  read_html()

```

Identify the CSS selectors in order to extract the relevant information from this page, namely

1.  job
2.  firm
3.  functional area
4.  type

Can you get all pages of ads, and not just the first one, `https://www.consultancy.uk/jobs/page/1` into a dataframe?

-   Write a function called `scrape_jobs()` that scrapes information from the webpage for consulting positions. This function should

    -   have one input: the URL of the webpage and should return a data frame with four columns (variables): job, firm, functional area, and type

    -   Test your function works with other pages too, e.g., <https://www.consultancy.uk/jobs/page/2>. Does the function seem to do what you expected it to do?

    -   Given that you have to scrape `...jobs/page/1`, `...jobs/page/2`, etc., define your URL so you can join multiple stings into one string, using `str_c()`. For instnace, if `page` is 5, what do you expect the following code to produce?

```         
base_url <- "https://www.consultancy.uk/jobs/page/1"
url <- str_c(base_url, page)
```

-   Construct a vector called `pages` that contains the numbers for each page available

-   Map the `scrape_jobs()` function over `pages` in a way that will result in a data frame called `all_consulting_jobs`.

-   Write the data frame to a csv file called `all_consulting_jobs.csv` in the `data` folder.
